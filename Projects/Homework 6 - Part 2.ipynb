{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data558spring2020/train_features.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c4d771bd9f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_features.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_features.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data558spring2020/train_features.npy'"
     ]
    }
   ],
   "source": [
    "data_dir = 'data558spring2020'\n",
    "\n",
    "# Load the data\n",
    "x_train = np.load(os.path.join(data_dir, 'train_features.npy'))\n",
    "y_train = np.load(os.path.join(data_dir, 'train_labels.npy'))\n",
    "x_valid = np.load(os.path.join(data_dir, 'val_features.npy'))\n",
    "y_valid = np.load(os.path.join(data_dir, 'val_labels.npy'))\n",
    "x_test = np.load(os.path.join(data_dir, 'test_features.npy'))\n",
    "# y_test = np.load(os.path.join(data_dir, 'test_labels.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train1[y_train1 < 2]\n",
    "y_train = y_train1[y_train1 < 2]\n",
    "x_valid = x_valid1[y_valid1 < 2]\n",
    "y_valid = y_valid1[y_valid1 < 2]\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.fit_transform(x_valid)\n",
    "x_test = scaler.fit_transform(x_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the number of samples and dimension of each sample \n",
    "n_train = len(y_train) \n",
    "n_test = x_test1.shape[0] \n",
    "d = np.size(x_train, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C= 0.005994842503189409\n",
      "Optimal lambda= 0.08340502686000296\n"
     ]
    }
   ],
   "source": [
    "lr_cv = LogisticRegressionCV(penalty='l2', fit_intercept=False, tol=10e-8, max_iter=1000, cv=5) \n",
    "lr_cv.fit(x_train, y_train) \n",
    "optimal_lambda = 1/(2*lr_cv.C_[0]*len(x_train)) \n",
    "print('Optimal C=', lr_cv.C_[0]) \n",
    "print('Optimal lambda=', optimal_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters to feed into the functions\n",
    "lamda = optimal_lambda\n",
    "L = (np.max(np.linalg.eig(1/len(y_train)*x_train.T@x_train)[0])) + lamda\n",
    "eta_0 = 1/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fx_logisticReg(b, lamda=0.5,X=x_train,y=y_train):\n",
    "    h = X @ b * y\n",
    "    return (1/len(y)) * sum(np.log(1 + np.exp(-h))) + (lamda * np.linalg.norm(b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(b, lamda,X=x_train,y=y_train):\n",
    "    h = X @ b * y \n",
    "    return 2*lamda*b - (1/len(y) * (X.T @ (y / (np.exp(h) + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(beta, lamda, X, y, eta=1, alpha=0.5, betaparam=0.8, maxiter=100): \n",
    "    grad_beta = computegrad(b=beta, lamda=lamda, X=X, y=y) \n",
    "    norm_grad_beta = np.linalg.norm(grad_beta) \n",
    "    found_eta = 0 \n",
    "    num_iters = 0 \n",
    "    while found_eta == 0 and num_iters < maxiter: \n",
    "        if Fx_logisticReg(beta - eta * grad_beta,lamda=lamda,X=X,y=y) <  Fx_logisticReg(beta, lamda=lamda, X=X, y=y) - alpha * eta * norm_grad_beta ** 2: \n",
    "            found_eta = 1 \n",
    "        elif num_iters == maxiter: \n",
    "            raise ('Max number of iterations of backtracking line search reached') \n",
    "        else: \n",
    "            eta *= betaparam \n",
    "            num_iters += 1 \n",
    "        \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myclassifier(eta, epsilon,lamda, X=x_train, y=y_train):\n",
    "    \"\"\"\n",
    "    Function for the fast gradient descent for the exponential loss algorithm\n",
    "    \"\"\"\n",
    "    eta_l = eta\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    beta_vals = beta\n",
    "    \n",
    "    grad_theta = computegrad(theta,lamda=lamda,X=X,y=y) \n",
    "    grad_beta = computegrad(beta,lamda=lamda,X=X,y=y)\n",
    "    \n",
    "    num_iters = 0\n",
    "    while np.linalg.norm(grad_beta) > epsilon:\n",
    "        eta_l = backtracking(beta=beta, lamda = lamda, X=X, y=y, eta=eta_l, alpha=0.5, betaparam=0.8, maxiter=100)\n",
    "        beta_new = theta - eta_l * grad_theta\n",
    "        theta = beta_new + (num_iters/(num_iters+3)) * (beta_new - beta)\n",
    "        \n",
    "        beta_vals = np.vstack((beta_vals, beta))\n",
    "        \n",
    "        grad_theta = computegrad(theta,lamda=lamda,X=X,y=y)\n",
    "        grad_beta = computegrad(beta,lamda=lamda,X=X,y=y)\n",
    "        \n",
    "        beta = beta_new\n",
    "        num_iters += 1\n",
    "        \n",
    "        \n",
    "    return beta_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_predicted(X,b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates Predicted y_hat values for the given X predictor variables and b weights\n",
    "    \"\"\"\n",
    "    y_hat = (X @ b)\n",
    "    y_pred = np.piecewise(y_hat,[y_hat >= threshold, y_hat <threshold],[lambda x: 1, lambda x: 0])\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_misclassification_error(b, X, y): \n",
    "    \"\"\"\n",
    "    Computes the missclassification error of the SVM for the calculated weights from the fast gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    -------------\n",
    "    y: response variables in the form of numpy.array\n",
    "    X: numpy.array of preditor variables \n",
    "    b: numpy.arry of the weights\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    The missclassification value of y_pred != y_actual\n",
    "    \"\"\"\n",
    "    y_pred = y_predicted(X=X, b=b)\n",
    "    return np.mean(y_pred != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_specificity(X, y_actual, b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculates Sensitivity and Specificity for the predicted y_hat values from\n",
    "    the provided X predictor variables and b weights compared to the y_actual\n",
    "    \"\"\"\n",
    "    yhat = y_predicted(X=X,b=b, threshold=threshold)\n",
    "    \n",
    "    TP = np.sum(yhat[yhat == 1] == y_actual[yhat==1])\n",
    "    TN = np.sum(yhat[yhat == 0] == y_actual[yhat==0])\n",
    "    FP = np.sum(yhat[yhat == 1] != y_actual[yhat==1])\n",
    "    FN = np.sum(yhat[yhat == 0] != y_actual[yhat==0])\n",
    "\n",
    "    sensitivity = TP/(TP+FN) \n",
    "    specificity = TN/(TN+FP)\n",
    "    \n",
    "    return [sensitivity, specificity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_misclassification_err(b, names=('training', 'validation'), xdata=(x_train, x_valid), ydata=(y_train, y_valid)):\n",
    "    for (name, x, y) in zip(names, xdata, ydata): \n",
    "        error = compute_misclassification_error(b, X=x, y=y) \n",
    "        sensitivity, specificity = sensitivity_specificity(X=x, y_actual=y, b=b)\n",
    "        print(name + ' data')\n",
    "        print('Misclassification error: %0.3f'% (100*error)) \n",
    "        print('Sensitivity of: %0.3f %%'% (100*sensitivity))\n",
    "        print('Specificity of: %0.3f \\n'% (100*specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "Misclassification error: 3.100\n",
      "Sensitivity of: 100.000 %\n",
      "Specificity of: 93.800 \n",
      "\n",
      "validation data\n",
      "Misclassification error: 5.500\n",
      "Sensitivity of: 98.000 %\n",
      "Specificity of: 91.000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the betas for the training set\n",
    "betas_tr = myclassifier(eta=eta_0, epsilon=5.1**-3, lamda=lamda)\n",
    "\n",
    "# printing values for classification error, sensitivity, and specificity\n",
    "print_misclassification_err(betas_tr[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted1 = y_predicted(x_test, betas_tr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.real(predicted1), columns=['Category'])\n",
    "submission.index.name='Id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

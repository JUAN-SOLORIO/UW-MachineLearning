{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression with Gradient Descent\n",
    "\n",
    "### Author: Juan Solorio\n",
    "\n",
    "-----\n",
    "\n",
    "# Overview\n",
    "In this exercise, I will implement a first version of my own gradient descent algorithm to\n",
    "solve the ridge regression problem. I will keep improving and extending this gradient descent optimization algorithm. In this notebook, I will implement a basic version of the algorithm.\n",
    "\n",
    "## Objectives\n",
    "- Mathematically define _Objective Function_ for Ridge Regression ($F\\beta$)\n",
    "    - Compute gradiant $\\nabla F$\n",
    "- Create functions for algorithm:\n",
    "    - Objective function\n",
    "    - Gradient function\n",
    "    - Gradient Descent funtion\n",
    "- Observe (plot) the convergence in terms of the function values and the gradients  \n",
    "    and tune for the optimal hyperparameters for step-size ($\\eta$) and normalization ($\\lambda$)\n",
    "- Compare to _sklearn_\n",
    "\n",
    "\n",
    "## Environment Setup - *Importing Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([1,2,3,4])\n",
    "arr2 = np.array([1,3,6,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.477225575051661, 5.477225575051661, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(arr1), np.sqrt(arr1 @ arr1), arr1.dot(arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Theory\n",
    "\n",
    "We start by defining the ___Linear Regression___ as a supervised learning algorithm and we write the mathematical algorithm as:\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "> y - target value  \n",
    "m - slope  \n",
    "x - predictor variable  \n",
    "b - intercept\n",
    "\n",
    "where $m$ and $b$ are the variables the algorithm will try to predict from the data.\n",
    "\n",
    "We can generalize this equation for multiple variables and write our _cost function_ to optimize our algorithm as:\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}n\\sum_{i=1}^n(y_i - h(x_i))^2 \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "h(x_i) = x_1\\beta_1 + x_2\\beta_2 + ... + x_{i,j}\\beta_j\n",
    "$$\n",
    "\n",
    "Where $h(x_i)$ is the predicted value for the function, $\\beta$ are the weights for the individual variables, and $y_i$ is the target value of the data.\n",
    "\n",
    "We can finalize this by writing the equation in the expanded form:\n",
    "$$\n",
    "F(\\beta) = \\frac{1}n\\sum_{i=1}^n(y_i - \\sum_{j=1}^dx_{ij}\\beta_j)^2\n",
    "$$\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "Linear Regression treats all the features equally and finds unbiased weights to minimizes the cost function.\n",
    "In _Ridge Regression_ , there is an addition of l2 penalty ( square of the magnitude of weights ) in the cost function of Linear Regression:\n",
    "\n",
    "\n",
    "$$\n",
    "\\lambda\\|\\beta\\|^2_2\n",
    "\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ is a hyperparameter to be tuned and this is done so that the model does not overfit the data.  \n",
    "\n",
    "We can write the final objective equation as:\n",
    "\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}n\\sum_{i=1}^n(y_i - \\sum_{j=1}^dx_{ij}\\beta_j)^2 + \\lambda\\sum_{j=1}^d\\beta_j^2\n",
    "$$\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters or weights ($\\beta$) of our model.\n",
    "\n",
    "In order to  minimize with respect to $\\beta$, we want to take the derivative of $F(\\beta)$ and set it equal to zero. \n",
    "\n",
    "The derivative for $n=1$ and $d=1$ yields:\n",
    "\\begin{equation}\n",
    "\\frac{dF}{d\\beta} = \\frac{2}n(y - x\\beta) + 2\\lambda\\beta\n",
    "\\end{equation}\n",
    "\n",
    "and for $n>1$ and $d>1$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} (\\frac{1}n\\sum_{i=1}^n(y_i - x^T_{i}\\beta_j)^2 + \\lambda\\|\\beta\\|^2_2)\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}n(\\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_j} (y_i - x^T_{i}\\beta_j)^2 + \\frac{\\partial}{\\partial \\beta_j} \\lambda\\|\\beta\\|^2_2)\n",
    "$$\n",
    "$$\n",
    "= -\\frac{2}n(\\sum_{i=1}^n x_{ij} (y_i - x^T_{i}\\beta_j)) + 2\\lambda\\sum_{j=1}^d\\beta_j\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "In Matrix terms, $F(\\beta)$ can be interpreted as:\n",
    "\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}{n}<Y - X\\beta,Y-X\\beta> + \\lambda\\|\\beta\\|^2_2\n",
    "$$\n",
    "\n",
    "\n",
    "Taking the derivative of the matrix form of $F(\\beta)$ leads us to the following:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla F = \\frac{\\partial}{\\partial\\beta} F= - \\frac{2}{n}X^T(y + X\\beta) + 2\\lambda\\beta\n",
    "$$\n",
    "\n",
    "\n",
    "Which is also known as the _gradient_ of the _objective function_ $F$.\n",
    "\n",
    "\n",
    "# Algorithm Functions\n",
    "\n",
    "* Objective function for Ridge Regression $F(\\beta)$:\n",
    "\n",
    ">$$\n",
    "F(\\beta) = \\frac{1}n\\sum_{i=1}^n(y_i - X^T_{ij} \\cdot \\beta_j)^2 + \\lambda\\|\\beta_j\\|^2_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fx(beta,lamda,X,y):\n",
    "    \"\"\"\n",
    "    Linear regression with Ridge penalty L1:\n",
    "    F(b) = 1/n sum((y - x dotproduct b)^2) + lamda*norm(b)^2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : arr\n",
    "        array of values for weights\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        computation of the objective function\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # dot product can be accomplish by 'numpy_arrayA @ numpy_arrayB' or 'numpy_arrayA.dot(numpy_arrayB)'\n",
    "    return 1/len(y) * sum((y - X @ beta)**2) + lamda*np.linalg.norm(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient of objective funtion - $\\nabla F$:\n",
    ">$$\n",
    "\\nabla F = \\frac{\\partial}{\\partial\\beta} F= - \\frac{2}{n}X^T(y + X\\beta) + 2\\lambda\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_fx(beta,lamda,X,y):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Linear regression with Ridge penalty L1 function:\n",
    "    grad F(b) = -2/n (x.T dotproduct (y - x dotproduct b)) + 2*lamda*b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : arr\n",
    "        array of values for weights\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        computation of the gradient of Ridge regression objective function\n",
    "\n",
    "    \"\"\"\n",
    "    return (-2/len(y)) * X.T @ (y - X @ beta) + 2*lamda*beta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient Descent Algorith:\n",
    ">`Gradient Descent algorithm with fixed constant step-size  \n",
    "__input__  step-size $\\eta$  \n",
    "__initialization__ $\\beta_0 = 0$  \n",
    "__repeat for__ t = 0, 1, 2, . . .  \n",
    "    $\\beta_{t+1} = \\beta_t âˆ’ \\eta \\nabla F(\\beta_t)$  \n",
    "__until__ the stopping criterion $\\|\\nabla F\\| \\leq \\epsilon$ is satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(beta_init, eta ,lamda,X,y,epsilon=0.005):\n",
    "    \"\"\"\n",
    "    Computes gradient descent with a fixed step size eta and stopping condition norm gradient F < epislon\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta_init : arr\n",
    "        array of values for weights as starting point\n",
    "    eta : int\n",
    "        interger value for step size parameter\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "    epsilon : int\n",
    "        interger value for stopping parameter condition, defaul = 0.005\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     beta_vals: Matrix \n",
    "         Estimated betas at each iteration, with the most recent values in the last row\n",
    "    \"\"\"\n",
    "    # gradient calculation for starting values\n",
    "    gradient = gradient_fx(beta,lamda,X,y)\n",
    "    # list to save beta values \n",
    "    beta_vals = [beta_init]\n",
    "    # loop for stopping criterion epsilon\n",
    "    while np.linalg.norm(gradient) > epsilon:\n",
    "        # updating values for beta and gradient\n",
    "        beta = beta - eta*gradient\n",
    "        gradient = gradient_fx(beta,lamda,X,y)\n",
    "        \n",
    "        # appending values\n",
    "        beta_vals.append(beta)\n",
    "    return np.array(beta_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data558] *",
   "language": "python",
   "name": "conda-env-data558-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

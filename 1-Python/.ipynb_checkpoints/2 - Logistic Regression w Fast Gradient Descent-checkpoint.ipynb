{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Fast Gradient Descent\n",
    "\n",
    "### Author: Juan Solorio\n",
    "\n",
    "-----\n",
    "\n",
    "# Overview\n",
    "In this exercise, I will implement a first version of my own own fast gradient algorithm to solve the $l^2_2$-regularized logistic regression problem. I will be building on the stocastic gradient descent algorithm and adding a backtracking function for the gradient descent algorithm.\n",
    "\n",
    "## Objectives\n",
    "- Mathematically define _Objective Function_ for Logistic Regression ($F(\\beta)$)\n",
    "    - Compute gradiant $\\nabla F$\n",
    "- Create functions for algorithm:\n",
    "    - Objective function\n",
    "    - Gradient function\n",
    "    - Fast Gradient Descent funtion & gradient descent function and compare\n",
    "    - Backtracking Line Search function\n",
    "    - Check misclassification error\n",
    "    \n",
    "- Observe (plot) the convergence in terms of the function values and the gradients  \n",
    "    and tune for the optimal hyperparameters for step-size ($\\eta$) and normalization ($\\lambda$)\n",
    "- Compare to _sklearn_\n",
    "\n",
    "\n",
    "## Environment Setup - *Importing Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Theory\n",
    "\n",
    "***logistic Regression*** is a type of _Supervised Machine Learning_ algorithm with the purpose to be used in ***binary classification***, as such this algorithm is confine to make predictions between {0,1} or {-1,1}. Given that we are trying to make this binary prediction of our target variable, we are dealing with a model trying to get variables where the probabilities are bounded on both ends (they must be between 0 and 1). \n",
    "\n",
    "We start with the traditional definition of the _Linear Regression_ formula:\n",
    "$$\n",
    "y = b_0*x + b_1\n",
    "$$\n",
    "> y - target value  \n",
    "$b_0$ - slope  \n",
    "x - predictor variable  \n",
    "$b_1$ - intercept\n",
    "\n",
    "However, given linear regression gives _unbounded_ solutions, we must _restrict_ the function by wrapping it around or passing it through a **link function**. We start by taking our linear function and defining it in terms of the statistical ***Odds*** of the given event occuring:\n",
    "> $P(Y=1 | X=x)$, read as the probability of $Y=1$ given that $X$ is already value $x$.  \n",
    "$Odds=\\frac{P}{1-P}$, the _Odds_ of something happening is the probability of it _happening_ devided by it _not happening_ .\n",
    "\n",
    "We then transform our linear model and Odds of occurance into our _logistic regression_ :\n",
    "$$\n",
    "Ln(\\frac{P}{1-P}) = b_0*x + b_1 \\\\\n",
    "\\Rightarrow \\frac{P}{1-P} = e^{b_0*x + b_1} \\\\\n",
    "\\Rightarrow P = e^{b_0*x + b_1} * (1-P) \\\\\n",
    "\\Rightarrow e^{b_0*x + b_1} = P + P*e^{b_0*x + b_1}  \\\\\n",
    "\\Rightarrow P = \\frac{e^{b_0*x + b_1}}{1+e^{b_0*x + b_1}} \\\\\n",
    "$$\n",
    "\n",
    "As we can observe this final result is what is known as passing our linear regression through the ***Sigmoid function***:\n",
    ">a mathematical function having a characteristic \"S\"-shaped curve or **sigmoid curve**\n",
    "$$\n",
    "S(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "We can generalize the *Logistic Regression* ***objective function*** as:\n",
    "$$\n",
    "F(\\beta) = \\min_{\\beta}F(\\beta) = \\frac{1}{n}\\sum_{i=1}^n log(1 + exp^{-y_ix^T_i\\beta}) + \\lambda\\|\\beta\\|^2_2\n",
    "$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta}F(\\beta) = \\frac{1}{n}\\sum_{i=1}^n log(1 + exp^{-y_i\\sum^d_{j=1}x^T_{i,j}\\beta_j}) + \\lambda\\sum_{j=1}^d\\beta_j^2\n",
    "\\end{equation}\n",
    "\n",
    "Using the machine learning convention for the labels that is $y_i\\ \\in$ {−1, +1}\n",
    "\n",
    "Assuming $d=1$ and $n=1$, we can compute the $\\nabla F$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\beta) = log(1 + exp^{-yx\\beta}) + \\lambda\\beta^2\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\Rightarrow \\frac{\\partial F}{\\partial \\beta} = \\frac{(-yx)exp^{-yxb}}{(1 + exp^{-yx\\beta})} + 2\\lambda\\beta\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\Rightarrow \\nabla F(\\beta) = 2\\lambda \\beta - \\frac{yx}{(1 + exp^{-yx\\beta})* exp^{yx\\beta}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\Rightarrow \\nabla F(\\beta) = 2\\lambda \\beta - \\frac{yx}{exp^{yx\\beta} + 1}\n",
    "\\end{equation}\n",
    "\n",
    "We then generalize for $d>1$ and $n>1$ as:\n",
    "$$\n",
    "\\nabla F(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n \\frac{yx}{exp^{yx\\beta} + 1} + 2\\lambda \\beta\n",
    "$$\n",
    "\n",
    "\n",
    "# Algorithm Function Definition\n",
    "\n",
    "* Objective function for _Logistic Regression_ $F(\\beta)$:\n",
    ">$$\n",
    "F(\\beta) = \\min_{\\beta}F(\\beta) = \\frac{1}{n}\\sum_{i=1}^n log(1 + exp^{-y_ix^T_i\\beta}) + \\lambda\\|\\beta\\|^2_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fx(beta,lamda,X,y):\n",
    "    \"\"\"\n",
    "    Linear regression with Ridge penalty L1:\n",
    "    F(b) = 1/n sum(log(1 + exp(-y*x dotproduct b))) + lamda*norm(b)^2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : arr\n",
    "        array of values for weights\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        computation of the logistic regression objective function\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # dot product can be accomplish by 'numpy_arrayA @ numpy_arrayB' or 'numpy_arrayA.dot(numpy_arrayB)'\n",
    "    return 1/len(y) * np.sum(np.log(1 + np.exp(-y*X.dot(beta)))) + lamda * np.linalg.norm(beta)**2\n",
    "\n",
    "def obj_fx_alt(b, lamda,X,y):\n",
    "    h = X @ b * y\n",
    "    return (1/len(y)) * sum(np.log(1 + np.exp(-h))) + (lamda * np.linalg.norm(b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient of the logistic regression objective function - $\\nabla F$\n",
    "> $$\\nabla F(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n \\frac{yx}{exp^{yx\\beta} + 1} + 2\\lambda \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_fx(beta,lamda,X,y):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Linear regression with Ridge penalty L1 function:\n",
    "    grad F(b) = -1/n sum(yx / (exp(y*x dotproduct b)+1)) + 2*lamda*b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : arr\n",
    "        array of values for weights\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        computation of the gradient of Logistic regression objective function\n",
    "\n",
    "    \"\"\"\n",
    "    yx = y[:, np.newaxis]*X\n",
    "    denom = 1+np.exp(-yx.dot(beta))\n",
    "    grad = 1/len(y)*np.sum(-yx*np.exp(-yx.dot(beta[:, np.newaxis]))/denom[:, np.newaxis], axis=0) + 2*lamda*beta\n",
    "    return grad\n",
    "\n",
    "def gradient_fx_alt(b, lamda,X,y):\n",
    "    h = X @ b * y \n",
    "    return 2*lamda*b - (1/len(y) * (X.T @ (y / (np.exp(h) + 1)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Backtracking algorithm\n",
    "> **initialize** $\\eta_t = \\eta_{t-1}$ if $t \\geq 1$  \n",
    "$\\eta_0 = \\frac{1}{L_{est}}$  \n",
    "**Iterate** $\\eta_t = \\gamma \\eta_t$  \n",
    "_Until condition met_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(beta, lamda, X, y, eta=1, alpha=0.5, gamma=0.8, maxiter=100):\n",
    "    \"\"\"\n",
    "    Computes backtracking linear search algorithm:\n",
    "    F(B_t − η∇F(B_t)) ≤ F(B_t) + αη||∇F(B_t)||^2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : arr\n",
    "        array of values for weights\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "    eta: int\n",
    "        integer value for step size parameter\n",
    "    alpha: int\n",
    "        integer value for stepsize change\n",
    "    gamma: int\n",
    "        integer value \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        computation of the change in eta\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    grad_beta = gradient_fx(beta, lamda, X, y)\n",
    "    norm_grad_beta = np.linalg.norm(grad_beta)\n",
    "    found_eta = 0\n",
    "    num_iters = 0\n",
    "    while found_eta == 0 and num_iters < maxiter:\n",
    "        if obj_fx(beta - eta * grad_beta, lamda, X,y) < obj_fx(beta, lamda, X, y) - alpha * eta * norm_grad_beta ** 2:\n",
    "            found_eta = 1\n",
    "        elif num_iters == maxiter:\n",
    "            raise ('Max number of iterations of backtracking line search reached')\n",
    "        else:\n",
    "            eta *= betaparam\n",
    "            num_iters += 1\n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient Descent Algorith:\n",
    ">`Gradient Descent algorithm with fixed constant step-size  \n",
    "__input__  step-size $\\eta$  \n",
    "__initialization__ $\\beta_0 = 0$  \n",
    "__repeat for__ t = 0, 1, 2, . . .  \n",
    "    $\\beta_{t+1} = \\beta_t − \\eta \\nabla F(\\beta_t)$  \n",
    "__until__ the stopping criterion $\\|\\nabla F\\| \\leq \\epsilon$ is satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(beta_init, eta_init ,lamda,X,y,epsilon=0.005):\n",
    "    \"\"\"\n",
    "    Computes gradient descent with a fixed step size eta and stopping condition norm gradient F < epislon\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta_init : arr\n",
    "        array of values for weights as starting point\n",
    "    eta_init : int\n",
    "        interger value for step size parameter\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "    epsilon : int\n",
    "        interger value for stopping parameter condition, defaul = 0.005\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     beta_vals: Matrix \n",
    "         Estimated betas at each iteration, with the most recent values in the last row\n",
    "    \"\"\"\n",
    "    # setting initial value of betas\n",
    "    beta = beta_init\n",
    "    eta = eta_init\n",
    "    # gradient calculation for starting values\n",
    "    gradient = gradient_fx(beta,lamda,X,y)\n",
    "    # list to save beta values \n",
    "    beta_vals = [beta_init]\n",
    "    # loop for stopping criterion epsilon\n",
    "    while np.linalg.norm(gradient) > epsilon:\n",
    "        # updating values for eta, beta and gradient\n",
    "        eta = backtracking_1(beta, lamda, eta=eta, X=X, y=y)\n",
    "        beta = beta - eta*gradient\n",
    "        gradient = gradient_fx(beta,lamda,X,y)\n",
    "        \n",
    "        # appending values\n",
    "        beta_vals.append(beta)\n",
    "    return np.array(beta_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fast Gradient Descent Algorithm:\n",
    "> **input** step-size $\\eta_0$, target accuracy $\\epsilon$  \n",
    "**initialization** $\\beta_0 = 0$, $\\theta_0 = 0$  \n",
    "**repeat** for $t = 0, 1, 2, . . .$  \n",
    "**Find** $\\eta_t$ with backtracking rule  \n",
    "    $\\beta_{t+1} = \\theta_t - \\eta_t \\nabla F(\\eta_t)$  \n",
    "    $\\theta_{t+1} = \\beta_{t+1} + \\frac{t}{t+3}(\\beta_{t+1} - \\beta_t)$  \n",
    "**until** the stopping criterion $||\\nabla F|| \\leq \\epsilon$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_algo(beta_init, theta_init, eta_init,lamda, X,y,epsilon=0.005):\n",
    "    \"\"\"\n",
    "    Computes Fast gradient descent with a initial conditions beta_init, theta_init, fixed step size eta\n",
    "    and stopping condition norm gradient F < epislon\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta_init : arr\n",
    "        array of values for weights as starting point\n",
    "    eta_init : int\n",
    "        interger value for step size parameter\n",
    "    theta_init : int\n",
    "        interger value for updating beta parameter\n",
    "    lamda : int\n",
    "        interger value for normalization parameter\n",
    "    X : arr\n",
    "        array of features from data\n",
    "    y : arr\n",
    "        array of target values from data\n",
    "    epsilon : int\n",
    "        interger value for stopping parameter condition, defaul = 0.005\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     beta_vals: Matrix \n",
    "         Estimated betas at each iteration, with the most recent values in the last row\n",
    "    \"\"\"\n",
    "    # setting initial value of betas\n",
    "    beta = beta_init\n",
    "    eta = eta_init\n",
    "    theta = theta_init\n",
    "    # gradient calculation for starting values\n",
    "    grad = gradient_fx(theta,lamda,X,y)\n",
    "    # list to save beta values \n",
    "    beta_vals = [beta_init]\n",
    "    num_iters = 0\n",
    "    # loop for stopping criterion epsilon\n",
    "    while np.linalg.norm(grad) > epsilon:\n",
    "        # updating values for eta, beta and gradient\n",
    "        eta = backtracking_1(beta, lamda, eta=eta, X=X, y=y)\n",
    "        beta_new = theta - eta*grad\n",
    "        theta = beta_new + num_iters/(num_iters+3)*(beta_new-beta)\n",
    "        \n",
    "        # appending values\n",
    "        beta = beta_new\n",
    "        beta_vals.append(beta)\n",
    "        \n",
    "        grad = gradient_fx(theta,lamda,X,y)\n",
    "       \n",
    "        num_iters += 1\n",
    "        \n",
    "    return np.array(beta_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "For testing our _logistic Regression_ algorithm, we'll be using the ___Spam___ dataset from `'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data'` for out features and `'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.traintest'` for our target variable, and we'll drop the samples with `NA` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9  ...     49   50  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000  0.0   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.132  0.0   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.143  0.0   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.137  0.0   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.135  0.0   \n",
       "\n",
       "      51     52     53     54   55    56  57  indicator  \n",
       "0  0.778  0.000  0.000  3.756   61   278   1          1  \n",
       "1  0.372  0.180  0.048  5.114  101  1028   1          0  \n",
       "2  0.276  0.184  0.010  9.821  485  2259   1          1  \n",
       "3  0.137  0.000  0.000  3.537   40   191   1          0  \n",
       "4  0.135  0.000  0.000  3.537   40   191   1          0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "spam = pd.read_table(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data\",sep=\" \", header=None)\n",
    "spam['indicator'] = pd.read_table(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.traintest\",header=None) \n",
    "spam = spam.dropna()\n",
    "\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our X matrix with the predictors and y vector with the response\n",
    "X = spam.drop(\"indicator\", axis=1)\n",
    "y = spam.indicator\n",
    "y.replace(0,-1,inplace=True)\n",
    "# Divide the data into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "# Standardize the data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "scaler = preprocessing.StandardScaler().fit(y_train.values.reshape(-1, 1))\n",
    "# Keep track of the number of samples and dimension of each sample\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to load into functions\n",
    "lamda = 0.5\n",
    "L = (np.max(np.linalg.eig(1/len(y_train)*X_train.T@X_train)[0])) + lamda\n",
    "eta_init = 1/L\n",
    "d = X_train.shape[1]\n",
    "beta_init = np.zeros(d)\n",
    "theta_init = np.zeros(d)\n",
    "maxiter = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02032379  0.0027978  -0.00584371  0.00202893 -0.00126049  0.00671368\n",
      " -0.00065947  0.00121773 -0.01000378  0.00207672 -0.00820628 -0.00858195\n",
      " -0.00094936 -0.01933799  0.0020736  -0.01503039 -0.00170232  0.00161148\n",
      "  0.00071245 -0.01061828  0.00350524  0.00459303  0.00163439 -0.00363655\n",
      " -0.00080175 -0.00234983  0.00323827 -0.00892796  0.01102586 -0.00145684\n",
      " -0.00163501  0.00535138 -0.00677281  0.005918   -0.00181521  0.00446524\n",
      "  0.00861823  0.00627413  0.00205909  0.00170645  0.00752408  0.00109195\n",
      "  0.00468198 -0.00098964  0.00613565 -0.0003137  -0.00225585 -0.00293253\n",
      " -0.00110998 -0.00093037  0.01690183  0.00284367  0.00756908  0.01537267\n",
      "  0.00131323  0.00384906  0.00171852  0.00559917]\n",
      "Total runtime of the program is 1.0137851238250732\n",
      "[ 0.02032379  0.0027978  -0.00584371  0.00202893 -0.00126049  0.00671368\n",
      " -0.00065947  0.00121773 -0.01000378  0.00207672 -0.00820628 -0.00858195\n",
      " -0.00094936 -0.01933799  0.0020736  -0.01503039 -0.00170232  0.00161148\n",
      "  0.00071245 -0.01061828  0.00350524  0.00459303  0.00163439 -0.00363655\n",
      " -0.00080175 -0.00234983  0.00323827 -0.00892796  0.01102586 -0.00145684\n",
      " -0.00163501  0.00535138 -0.00677281  0.005918   -0.00181521  0.00446524\n",
      "  0.00861823  0.00627413  0.00205909  0.00170645  0.00752408  0.00109195\n",
      "  0.00468198 -0.00098964  0.00613565 -0.0003137  -0.00225585 -0.00293253\n",
      " -0.00110998 -0.00093037  0.01690183  0.00284367  0.00756908  0.01537267\n",
      "  0.00131323  0.00384906  0.00171852  0.00559917]\n",
      "Total runtime of the program is 1.010300874710083\n"
     ]
    }
   ],
   "source": [
    "# store starting time \n",
    "begin = time.time()\n",
    "print(gradient_fx(beta=beta_init,lamda=lamda,X=X_train,y=y_train))\n",
    "time.sleep(1) \n",
    "# store end time \n",
    "end = time.time() \n",
    "# total time taken \n",
    "print(f\"Total runtime of the program is {end - begin}\") \n",
    "\n",
    "# store starting time \n",
    "begin = time.time()\n",
    "print(computegrad(b=beta_init, lamda=lamda,X=X_train,y=y_train))\n",
    "time.sleep(1) \n",
    "# store end time \n",
    "end = time.time() \n",
    "# total time taken \n",
    "print(f\"Total runtime of the program is {end - begin}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "Total runtime of the program is 1.0062980651855469\n",
      "0.6931471805599352\n",
      "Total runtime of the program is 1.008281946182251\n"
     ]
    }
   ],
   "source": [
    "# store starting time \n",
    "begin = time.time()\n",
    "print(obj_fx(beta=beta_init,lamda=lamda,X=X_train,y=y_train))\n",
    "time.sleep(1) \n",
    "# store end time \n",
    "end = time.time() \n",
    "# total time taken \n",
    "print(f\"Total runtime of the program is {end - begin}\") \n",
    "\n",
    "# store starting time \n",
    "begin = time.time()\n",
    "print(Fx_logisticReg(b=beta_init, lamda=lamda,X=X_train,y=y_train))\n",
    "time.sleep(1) \n",
    "# store end time \n",
    "end = time.time() \n",
    "# total time taken \n",
    "print(f\"Total runtime of the program is {end - begin}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime of the program is 1.54715895652771\n",
      "Total runtime of the program is 1.277160406112671\n"
     ]
    }
   ],
   "source": [
    "# Initializing gradient decent and fast algorithm grad descent\n",
    "begin = time.time()\n",
    "graddesc = gradient_descent(beta_init, eta_init ,lamda,X=X_train,y=y_train,epsilon=0.005)\n",
    "time.sleep(1) \n",
    "# store end time \n",
    "end = time.time() \n",
    "# total time taken \n",
    "print(f\"Total runtime of the program is {end - begin}\")\n",
    "\n",
    "# store starting time \n",
    "begin = time.time()\n",
    "fastAlgo = fast_gradient_algo(beta_init, theta_init, eta_init,lamda, X=X_train,y=y_train,epsilon=0.005)\n",
    "time.sleep(1) \n",
    "# store end time \n",
    "end = time.time() \n",
    "# total time taken \n",
    "print(f\"Total runtime of the program is {end - begin}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14, 58), (7, 58))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graddesc.shape, fastAlgo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def objective_plot(betas_gd, betas_fg, lambduh, x=X_train, y=y_train):\n",
    "    num_points_gd = np.size(betas_gd, 0)\n",
    "    objs_gd = np.zeros(num_points_gd)\n",
    "    num_points_fg = np.size(betas_fg, 0)\n",
    "    objs_fg = np.zeros(num_points_fg)\n",
    "    obj_fx(betas_fg[0, :], lambduh, X=x, y=y)\n",
    "    for i in range(num_points_gd):\n",
    "        objs_gd[i] = obj_fx(betas_gd[i, :], lambduh, X=x, y=y)\n",
    "    for i in range(num_points_fg):\n",
    "        objs_fg[i] = obj_fx(betas_fg[i, :], lambduh, X=x, y=y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1, num_points_gd + 1), objs_gd,label='gradient descent')\n",
    "    ax.plot(range(1, num_points_fg + 1), objs_fg,label='fast gradient')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Objective value')\n",
    "    plt.title('Objective value vs. iteration when lambda='+str(lambduh))\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "objective_plot(graddesc, fastAlgo, lamda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data558] *",
   "language": "python",
   "name": "conda-env-data558-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
